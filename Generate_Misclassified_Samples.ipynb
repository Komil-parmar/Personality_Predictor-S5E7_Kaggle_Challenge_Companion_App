{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "982d69bb",
   "metadata": {},
   "source": [
    "# Generate Misclassified Samples for Streamlit App\n",
    "\n",
    "This notebook identifies misclassified samples from the training data and calculates the average distance between correctly classified points. The results will be used in the Streamlit app to show users how close they are to problematic outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a201f94c",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de274805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cabd1ad",
   "metadata": {},
   "source": [
    "## 2. Load Model and Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d3514db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Loading model components...\n",
      "âœ… Model loaded with accuracy: 0.9714\n",
      "Feature columns: ['Time_spent_Alone', 'Stage_fear', 'Social_event_attendance', 'Going_outside', 'Drained_after_socializing', 'Friends_circle_size', 'Post_frequency']\n",
      "Categorical columns: ['Stage_fear', 'Drained_after_socializing']\n",
      "Numerical columns: ['Time_spent_Alone', 'Social_event_attendance', 'Going_outside', 'Friends_circle_size', 'Post_frequency']\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model components\n",
    "print(\"ðŸ“‚ Loading model components...\")\n",
    "with open('xgboost_personality_model.pkl', 'rb') as f:\n",
    "    components = pickle.load(f)\n",
    "\n",
    "model = components['model']\n",
    "target_encoder = components['target_encoder']\n",
    "label_encoders = components['label_encoders']\n",
    "feature_columns = components['feature_columns']\n",
    "categorical_columns = components['categorical_columns']\n",
    "numerical_columns = components['numerical_columns']\n",
    "feature_stats = components['feature_stats']\n",
    "accuracy = components['accuracy']\n",
    "\n",
    "print(f\"âœ… Model loaded with accuracy: {accuracy:.4f}\")\n",
    "print(f\"Feature columns: {feature_columns}\")\n",
    "print(f\"Categorical columns: {categorical_columns}\")\n",
    "print(f\"Numerical columns: {numerical_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39b3769b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‚ Loading training data...\n",
      "âœ… Training data loaded: (18524, 25)\n",
      "Columns: ['id', 'Time_spent_Alone', 'Stage_fear', 'Social_event_attendance', 'Going_outside', 'Drained_after_socializing', 'Friends_circle_size', 'Post_frequency', 'Personality', 'xgb_Friends_circle_size_indicator', 'xgb_Friends_circle_size_oof', 'xgb_Social_event_attendance_indicator', 'xgb_Social_event_attendance_oof', 'xgb_Drained_after_socializing_indicator', 'xgb_Drained_after_socializing_oof', 'xgb_Time_spent_Alone_indicator', 'xgb_Time_spent_Alone_oof', 'xgb_Post_frequency_indicator', 'xgb_Post_frequency_oof', 'xgb_Going_outside_indicator', 'xgb_Going_outside_oof', 'xgb_Stage_fear_indicator', 'xgb_Stage_fear_oof', 'xgb_Personality_indicator', 'xgb_Personality_oof']\n",
      "\n",
      "Personality distribution:\n",
      "Personality\n",
      "Extrovert    13699\n",
      "Introvert     4825\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "print(\"\\nðŸ“‚ Loading training data...\")\n",
    "train_df = pd.read_csv('train_imputed_xgb.csv')\n",
    "print(f\"âœ… Training data loaded: {train_df.shape}\")\n",
    "print(f\"Columns: {list(train_df.columns)}\")\n",
    "print(f\"\\nPersonality distribution:\")\n",
    "print(train_df['Personality'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17680c53",
   "metadata": {},
   "source": [
    "## 3. Prepare Features and Identify Misclassified Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac781daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Preparing features...\n",
      "Original feature types:\n",
      "  Time_spent_Alone: float64 - Sample values: [0. 1. 6. 3. 2.]\n",
      "  Stage_fear: object - Sample values: ['No' 'Yes']\n",
      "  Social_event_attendance: float64 - Sample values: [6. 7. 1. 4. 8.]\n",
      "  Going_outside: float64 - Sample values: [4.00000000e+00 3.00000000e+00 0.00000000e+00 5.00000000e+00\n",
      " 4.35828697e-04]\n",
      "  Drained_after_socializing: object - Sample values: ['No' 'Yes']\n",
      "  Friends_circle_size: float64 - Sample values: [15. 10.  3. 11. 13.]\n",
      "  Post_frequency: float64 - Sample values: [5.         8.         0.         0.02070745 3.        ]\n"
     ]
    }
   ],
   "source": [
    "# Prepare training features\n",
    "print(\"ðŸ”§ Preparing features...\")\n",
    "X_train = train_df[feature_columns].copy()\n",
    "y_train = train_df['Personality'].copy()\n",
    "\n",
    "print(f\"Original feature types:\")\n",
    "for col in feature_columns:\n",
    "    print(f\"  {col}: {X_train[col].dtype} - Sample values: {X_train[col].unique()[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f3354ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ Encoding categorical variables...\n",
      "Encoding Stage_fear: ['No' 'Yes'] -> [0 1]\n",
      "Encoding Drained_after_socializing: ['No' 'Yes'] -> [0 1]\n",
      "\n",
      "âœ… Features encoded. Shape: (18524, 7)\n"
     ]
    }
   ],
   "source": [
    "# Encode categorical variables\n",
    "print(\"\\nðŸ”§ Encoding categorical variables...\")\n",
    "X_train_encoded = X_train.copy()\n",
    "\n",
    "for col in categorical_columns:\n",
    "    if col in X_train_encoded.columns and col in label_encoders:\n",
    "        le = label_encoders[col]\n",
    "        print(f\"Encoding {col}: {X_train_encoded[col].unique()} -> \", end=\"\")\n",
    "        X_train_encoded[col] = le.transform(X_train_encoded[col].astype(str))\n",
    "        print(f\"{X_train_encoded[col].unique()}\")\n",
    "\n",
    "print(f\"\\nâœ… Features encoded. Shape: {X_train_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "026ced73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ Encoding target variable...\n",
      "Target classes: ['Extrovert' 'Introvert']\n",
      "Encoded target distribution: [13699  4825]\n"
     ]
    }
   ],
   "source": [
    "# Encode target variable\n",
    "print(\"\\nðŸ”§ Encoding target variable...\")\n",
    "y_train_encoded = target_encoder.transform(y_train)\n",
    "print(f\"Target classes: {target_encoder.classes_}\")\n",
    "print(f\"Encoded target distribution: {np.bincount(y_train_encoded)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b301b61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ¤– Making predictions on training data...\n",
      "Predictions made. Shape: (18524,)\n",
      "Prediction distribution: [13790  4734]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on training data\n",
    "print(\"\\nðŸ¤– Making predictions on training data...\")\n",
    "y_pred = model.predict(X_train_encoded)\n",
    "y_pred_proba = model.predict_proba(X_train_encoded)\n",
    "\n",
    "print(f\"Predictions made. Shape: {y_pred.shape}\")\n",
    "print(f\"Prediction distribution: {np.bincount(y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8e00820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸŽ¯ Identifying misclassified samples...\n",
      "ðŸ“Š Classification Results:\n",
      "  Total samples: 18524\n",
      "  Correctly classified: 17933 (96.8%)\n",
      "  Misclassified: 591 (3.2%)\n",
      "  Accuracy: 0.9681\n"
     ]
    }
   ],
   "source": [
    "# Identify misclassified and correctly classified samples\n",
    "print(\"\\nðŸŽ¯ Identifying misclassified samples...\")\n",
    "misclassified_mask = y_train_encoded != y_pred\n",
    "correctly_classified_mask = y_train_encoded == y_pred\n",
    "\n",
    "num_misclassified = np.sum(misclassified_mask)\n",
    "num_correctly_classified = np.sum(correctly_classified_mask)\n",
    "total_samples = len(y_train_encoded)\n",
    "\n",
    "print(f\"ðŸ“Š Classification Results:\")\n",
    "print(f\"  Total samples: {total_samples}\")\n",
    "print(f\"  Correctly classified: {num_correctly_classified} ({num_correctly_classified/total_samples*100:.1f}%)\")\n",
    "print(f\"  Misclassified: {num_misclassified} ({num_misclassified/total_samples*100:.1f}%)\")\n",
    "print(f\"  Accuracy: {num_correctly_classified/total_samples:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8b7084",
   "metadata": {},
   "source": [
    "## 4. Extract Misclassified Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ccda7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Extracting misclassified samples...\n",
      "âœ… Misclassified samples extracted: 591 samples\n",
      "\n",
      "Sample of misclassified data:\n",
      "   original_index true_label predicted_label  confidence  Time_spent_Alone  \\\n",
      "0              63  Extrovert       Introvert    0.821004               8.0   \n",
      "1             102  Extrovert       Introvert    0.901223               6.0   \n",
      "2             124  Introvert       Extrovert    0.890060               4.0   \n",
      "3             139  Extrovert       Introvert    0.818464              11.0   \n",
      "4             149  Introvert       Extrovert    0.892519               0.0   \n",
      "\n",
      "   Stage_fear  Social_event_attendance  Going_outside  \\\n",
      "0         1.0                      4.0            2.0   \n",
      "1         1.0                      2.0            0.0   \n",
      "2         0.0                      7.0            6.0   \n",
      "3         1.0                      2.0            2.0   \n",
      "4         0.0                      4.0            6.0   \n",
      "\n",
      "   Drained_after_socializing  Friends_circle_size  Post_frequency  \n",
      "0                        1.0             0.008009             0.0  \n",
      "1                        1.0             2.000000             0.0  \n",
      "2                        0.0            11.000000             3.0  \n",
      "3                        1.0             2.000000             2.0  \n",
      "4                        0.0             0.036396             3.0  \n",
      "\n",
      "âœ… No NaN values in misclassified data\n"
     ]
    }
   ],
   "source": [
    "# Get misclassified samples with additional information\n",
    "print(\"\\nðŸ“Š Extracting misclassified samples...\")\n",
    "\n",
    "misclassified_data = []\n",
    "misclassified_indices = np.where(misclassified_mask)[0]\n",
    "\n",
    "for idx in misclassified_indices:\n",
    "    try:\n",
    "        # Check if the sample has valid data\n",
    "        sample_features = X_train_encoded.iloc[idx][feature_columns]\n",
    "        if sample_features.isnull().any():\n",
    "            print(f\"âš ï¸ Skipping sample {idx} due to missing values\")\n",
    "            continue\n",
    "            \n",
    "        sample_info = {\n",
    "            'original_index': train_df.iloc[idx]['id'] if 'id' in train_df.columns else idx,\n",
    "            'true_label': y_train.iloc[idx],\n",
    "            'predicted_label': target_encoder.inverse_transform([y_pred[idx]])[0],\n",
    "            'confidence': np.max(y_pred_proba[idx]),\n",
    "        }\n",
    "        \n",
    "        # Add all features\n",
    "        for feature in feature_columns:\n",
    "            feature_value = X_train_encoded.iloc[idx][feature]\n",
    "            # Handle potential NaN values\n",
    "            if pd.isna(feature_value):\n",
    "                print(f\"âš ï¸ NaN value found in feature {feature} for sample {idx}\")\n",
    "                feature_value = 0  # or use median/mode\n",
    "            sample_info[feature] = feature_value\n",
    "        \n",
    "        misclassified_data.append(sample_info)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing sample {idx}: {e}\")\n",
    "        continue\n",
    "\n",
    "misclassified_df = pd.DataFrame(misclassified_data)\n",
    "print(f\"âœ… Misclassified samples extracted: {len(misclassified_df)} samples\")\n",
    "\n",
    "if len(misclassified_df) > 0:\n",
    "    print(f\"\\nSample of misclassified data:\")\n",
    "    print(misclassified_df.head())\n",
    "    \n",
    "    # Check for any remaining NaN values\n",
    "    nan_check = misclassified_df.isnull().sum()\n",
    "    if nan_check.any():\n",
    "        print(f\"\\nâš ï¸ NaN values in misclassified data:\")\n",
    "        print(nan_check[nan_check > 0])\n",
    "    else:\n",
    "        print(\"\\nâœ… No NaN values in misclassified data\")\n",
    "else:\n",
    "    print(\"âŒ No valid misclassified samples found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33a35f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Analyzing misclassification patterns...\n",
      "Misclassification by true label:\n",
      "true_label\n",
      "Introvert    341\n",
      "Extrovert    250\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Misclassification by predicted label:\n",
      "predicted_label\n",
      "Extrovert    341\n",
      "Introvert    250\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Confidence distribution of misclassified samples:\n",
      "Mean confidence: 0.875\n",
      "Median confidence: 0.930\n",
      "Min confidence: 0.502\n",
      "Max confidence: 0.998\n"
     ]
    }
   ],
   "source": [
    "# Analyze misclassification patterns\n",
    "print(\"\\nðŸ” Analyzing misclassification patterns...\")\n",
    "\n",
    "print(\"Misclassification by true label:\")\n",
    "print(misclassified_df['true_label'].value_counts())\n",
    "\n",
    "print(\"\\nMisclassification by predicted label:\")\n",
    "print(misclassified_df['predicted_label'].value_counts())\n",
    "\n",
    "print(\"\\nConfidence distribution of misclassified samples:\")\n",
    "print(f\"Mean confidence: {misclassified_df['confidence'].mean():.3f}\")\n",
    "print(f\"Median confidence: {misclassified_df['confidence'].median():.3f}\")\n",
    "print(f\"Min confidence: {misclassified_df['confidence'].min():.3f}\")\n",
    "print(f\"Max confidence: {misclassified_df['confidence'].max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d2c00a",
   "metadata": {},
   "source": [
    "## 5. Calculate Average Distance Between Correctly Classified Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ebc84d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Inspecting encoded data quality...\n",
      "Encoded data shape: (18524, 7)\n",
      "Data types:\n",
      "Time_spent_Alone             float64\n",
      "Stage_fear                     int32\n",
      "Social_event_attendance      float64\n",
      "Going_outside                float64\n",
      "Drained_after_socializing      int32\n",
      "Friends_circle_size          float64\n",
      "Post_frequency               float64\n",
      "dtype: object\n",
      "\n",
      "Data summary:\n",
      "       Time_spent_Alone    Stage_fear  Social_event_attendance  Going_outside  \\\n",
      "count      18524.000000  18524.000000             18524.000000   18524.000000   \n",
      "mean           2.936211      0.257018                 4.931425       3.724326   \n",
      "std            3.005802      0.437001                 2.955463       2.260321   \n",
      "min           -0.001171      0.000000                -0.007918      -0.000364   \n",
      "25%            1.000000      0.000000                 3.000000       2.000000   \n",
      "50%            2.000000      0.000000                 5.000000       4.000000   \n",
      "75%            4.000000      1.000000                 7.000000       6.000000   \n",
      "max           11.000000      1.000000                10.000000       7.000000   \n",
      "\n",
      "       Drained_after_socializing  Friends_circle_size  Post_frequency  \n",
      "count               18524.000000         18524.000000    18524.000000  \n",
      "mean                    0.257072             7.544706        4.642903  \n",
      "std                     0.437031             4.495731        3.048762  \n",
      "min                     0.000000            -0.093505       -0.125592  \n",
      "25%                     0.000000             4.000000        2.000000  \n",
      "50%                     0.000000             8.000000        5.000000  \n",
      "75%                     1.000000            11.000000        7.000000  \n",
      "max                     1.000000            15.000000       10.000000  \n",
      "\n",
      "âœ… No missing values found\n",
      "\n",
      "âœ… No infinite values found\n",
      "\n",
      "Data ranges:\n",
      "  Time_spent_Alone: [-0.00, 11.00]\n",
      "  Stage_fear: [0 1]\n",
      "  Social_event_attendance: [-0.01, 10.00]\n",
      "  Going_outside: [-0.00, 7.00]\n",
      "  Drained_after_socializing: [0 1]\n",
      "  Friends_circle_size: [-0.09, 15.00]\n",
      "  Post_frequency: [-0.13, 10.00]\n"
     ]
    }
   ],
   "source": [
    "# Inspect the encoded data for any issues\n",
    "print(\"\\nðŸ” Inspecting encoded data quality...\")\n",
    "print(f\"Encoded data shape: {X_train_encoded.shape}\")\n",
    "print(f\"Data types:\\n{X_train_encoded.dtypes}\")\n",
    "print(f\"\\nData summary:\")\n",
    "print(X_train_encoded.describe())\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = X_train_encoded.isnull().sum()\n",
    "if missing_values.any():\n",
    "    print(f\"\\nâš ï¸ Missing values found:\")\n",
    "    print(missing_values[missing_values > 0])\n",
    "else:\n",
    "    print(\"\\nâœ… No missing values found\")\n",
    "\n",
    "# Check for infinite values\n",
    "infinite_values = np.isinf(X_train_encoded.select_dtypes(include=[np.number])).sum()\n",
    "if infinite_values.any():\n",
    "    print(f\"\\nâš ï¸ Infinite values found:\")\n",
    "    print(infinite_values[infinite_values > 0])\n",
    "else:\n",
    "    print(\"\\nâœ… No infinite values found\")\n",
    "\n",
    "# Check data ranges\n",
    "print(f\"\\nData ranges:\")\n",
    "for col in X_train_encoded.columns:\n",
    "    if X_train_encoded[col].dtype in ['int64', 'float64']:\n",
    "        print(f\"  {col}: [{X_train_encoded[col].min():.2f}, {X_train_encoded[col].max():.2f}]\")\n",
    "    else:\n",
    "        print(f\"  {col}: {X_train_encoded[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65af13b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Calculating average distance between correctly classified points...\n",
      "Number of correctly classified samples: 17933\n",
      "Checking for data quality issues...\n",
      "  NaN values: 0\n",
      "  Infinite values: 0\n",
      "Sampling 1000 points from 17933 for efficiency...\n",
      "Calculating pairwise distances for 1000 points...\n",
      "\n",
      "ðŸ“Š Distance Statistics for Correctly Classified Points:\n",
      "  Average distance: 9.4807\n",
      "  Median distance: 9.0339\n",
      "  Standard deviation: 4.0260\n",
      "  Number of distance pairs: 499499\n"
     ]
    }
   ],
   "source": [
    "# Calculate average distance between correctly classified points\n",
    "print(\"\\nðŸ“ Calculating average distance between correctly classified points...\")\n",
    "\n",
    "correctly_classified_samples = X_train_encoded[correctly_classified_mask].values\n",
    "print(f\"Number of correctly classified samples: {len(correctly_classified_samples)}\")\n",
    "\n",
    "# Check for NaN or infinite values\n",
    "print(f\"Checking for data quality issues...\")\n",
    "print(f\"  NaN values: {np.isnan(correctly_classified_samples).sum()}\")\n",
    "print(f\"  Infinite values: {np.isinf(correctly_classified_samples).sum()}\")\n",
    "\n",
    "# Clean the data - remove any rows with NaN or infinite values\n",
    "if np.isnan(correctly_classified_samples).any() or np.isinf(correctly_classified_samples).any():\n",
    "    print(\"âš ï¸ Found NaN or infinite values, cleaning data...\")\n",
    "    valid_mask = ~np.isnan(correctly_classified_samples).any(axis=1) & ~np.isinf(correctly_classified_samples).any(axis=1)\n",
    "    correctly_classified_samples = correctly_classified_samples[valid_mask]\n",
    "    print(f\"After cleaning: {len(correctly_classified_samples)} samples remaining\")\n",
    "\n",
    "if len(correctly_classified_samples) > 1:\n",
    "    # Sample a subset for efficiency if too many samples\n",
    "    if len(correctly_classified_samples) > 1000:\n",
    "        print(f\"Sampling 1000 points from {len(correctly_classified_samples)} for efficiency...\")\n",
    "        np.random.seed(42)  # For reproducibility\n",
    "        sample_indices = np.random.choice(len(correctly_classified_samples), 1000, replace=False)\n",
    "        sample_points = correctly_classified_samples[sample_indices]\n",
    "    else:\n",
    "        sample_points = correctly_classified_samples\n",
    "    \n",
    "    print(f\"Calculating pairwise distances for {len(sample_points)} points...\")\n",
    "    \n",
    "    # Double-check for any remaining NaN values\n",
    "    if np.isnan(sample_points).any() or np.isinf(sample_points).any():\n",
    "        print(\"âŒ Still have NaN/infinite values after cleaning\")\n",
    "        print(\"Sample point statistics:\")\n",
    "        print(f\"  Shape: {sample_points.shape}\")\n",
    "        print(f\"  Min: {np.nanmin(sample_points)}\")\n",
    "        print(f\"  Max: {np.nanmax(sample_points)}\")\n",
    "        print(f\"  NaN count: {np.isnan(sample_points).sum()}\")\n",
    "        print(f\"  Inf count: {np.isinf(sample_points).sum()}\")\n",
    "        avg_correct_distance = 0\n",
    "    else:\n",
    "        try:\n",
    "            # Calculate pairwise distances\n",
    "            distances = euclidean_distances(sample_points)\n",
    "            \n",
    "            # Get upper triangle (excluding diagonal) to avoid counting same pair twice\n",
    "            upper_triangle = np.triu(distances, k=1)\n",
    "            non_zero_distances = upper_triangle[upper_triangle > 0]\n",
    "            \n",
    "            avg_correct_distance = np.mean(non_zero_distances)\n",
    "            median_correct_distance = np.median(non_zero_distances)\n",
    "            std_correct_distance = np.std(non_zero_distances)\n",
    "            \n",
    "            print(f\"\\nðŸ“Š Distance Statistics for Correctly Classified Points:\")\n",
    "            print(f\"  Average distance: {avg_correct_distance:.4f}\")\n",
    "            print(f\"  Median distance: {median_correct_distance:.4f}\")\n",
    "            print(f\"  Standard deviation: {std_correct_distance:.4f}\")\n",
    "            print(f\"  Number of distance pairs: {len(non_zero_distances)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error calculating distances: {e}\")\n",
    "            avg_correct_distance = 0\n",
    "    \n",
    "else:\n",
    "    avg_correct_distance = 0\n",
    "    print(\"âš ï¸ Not enough correctly classified samples to calculate distances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa99451f",
   "metadata": {},
   "source": [
    "## 6. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d05f83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Misclassified samples saved to 'misclassified_samples.csv'\n",
      "File contains 591 rows and 11 columns\n",
      "Columns: ['original_index', 'true_label', 'predicted_label', 'confidence', 'Time_spent_Alone', 'Stage_fear', 'Social_event_attendance', 'Going_outside', 'Drained_after_socializing', 'Friends_circle_size', 'Post_frequency']\n"
     ]
    }
   ],
   "source": [
    "# Save misclassified samples to CSV\n",
    "misclassified_filename = 'misclassified_samples.csv'\n",
    "misclassified_df.to_csv(misclassified_filename, index=False)\n",
    "print(f\"âœ… Misclassified samples saved to '{misclassified_filename}'\")\n",
    "print(f\"File contains {len(misclassified_df)} rows and {len(misclassified_df.columns)} columns\")\n",
    "print(f\"Columns: {list(misclassified_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb23702d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§¹ Cleaning misclassified features data...\n",
      "Before cleaning:\n",
      "  NaN values: 0\n",
      "  Infinite values: 0\n",
      "\n",
      "After initial cleaning:\n",
      "  NaN values: 0\n",
      "  Infinite values: 0\n",
      "\n",
      "Final verification:\n",
      "  NaN values: 0\n",
      "  Infinite values: 0\n",
      "âœ… Cleaned misclassified features saved to 'misclassified_features.csv'\n",
      "File contains 591 rows and 7 columns\n",
      "Feature columns: ['Time_spent_Alone', 'Stage_fear', 'Social_event_attendance', 'Going_outside', 'Drained_after_socializing', 'Friends_circle_size', 'Post_frequency']\n"
     ]
    }
   ],
   "source": [
    "# Save only the feature values for distance calculations (no metadata)\n",
    "misclassified_features_only = misclassified_df[feature_columns].copy()\n",
    "\n",
    "# Clean the misclassified features data\n",
    "print(f\"\\nðŸ§¹ Cleaning misclassified features data...\")\n",
    "print(f\"Before cleaning:\")\n",
    "print(f\"  NaN values: {misclassified_features_only.isnull().sum().sum()}\")\n",
    "print(f\"  Infinite values: {np.isinf(misclassified_features_only.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "\n",
    "# Handle NaN values - fill with global median/mode from the entire training set\n",
    "for col in feature_columns:\n",
    "    if misclassified_features_only[col].isnull().any():\n",
    "        if col in numerical_columns:\n",
    "            # Use median from the entire training set\n",
    "            global_median = X_train_encoded[col].median()\n",
    "            if pd.isna(global_median):\n",
    "                global_median = 0\n",
    "            misclassified_features_only[col].fillna(global_median, inplace=True)\n",
    "            print(f\"  Filled {misclassified_features_only[col].isnull().sum()} NaN values in {col} with global median: {global_median}\")\n",
    "        else:\n",
    "            # Use mode from the entire training set\n",
    "            global_mode = X_train_encoded[col].mode()\n",
    "            if len(global_mode) > 0:\n",
    "                mode_val = global_mode[0]\n",
    "            else:\n",
    "                mode_val = 0\n",
    "            misclassified_features_only[col].fillna(mode_val, inplace=True)\n",
    "            print(f\"  Filled {misclassified_features_only[col].isnull().sum()} NaN values in {col} with global mode: {mode_val}\")\n",
    "\n",
    "# Handle infinite values - replace with 0\n",
    "numerical_cols = misclassified_features_only.select_dtypes(include=[np.number]).columns\n",
    "for col in numerical_cols:\n",
    "    inf_mask = np.isinf(misclassified_features_only[col])\n",
    "    if inf_mask.any():\n",
    "        misclassified_features_only.loc[inf_mask, col] = 0\n",
    "        print(f\"  Replaced {inf_mask.sum()} infinite values in {col} with 0\")\n",
    "\n",
    "# Final check and force cleanup of any remaining NaN/inf values\n",
    "print(f\"\\nAfter initial cleaning:\")\n",
    "print(f\"  NaN values: {misclassified_features_only.isnull().sum().sum()}\")\n",
    "print(f\"  Infinite values: {np.isinf(misclassified_features_only.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "\n",
    "# Force fill any remaining NaN values with 0\n",
    "if misclassified_features_only.isnull().sum().sum() > 0:\n",
    "    print(\"  Final cleanup: filling any remaining NaN values with 0\")\n",
    "    misclassified_features_only.fillna(0, inplace=True)\n",
    "\n",
    "# Force replace any remaining infinite values with 0\n",
    "for col in numerical_cols:\n",
    "    misclassified_features_only[col] = misclassified_features_only[col].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "print(f\"\\nFinal verification:\")\n",
    "print(f\"  NaN values: {misclassified_features_only.isnull().sum().sum()}\")\n",
    "print(f\"  Infinite values: {np.isinf(misclassified_features_only.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "\n",
    "# Save the cleaned features\n",
    "features_filename = 'misclassified_features.csv'\n",
    "misclassified_features_only.to_csv(features_filename, index=False)\n",
    "print(f\"âœ… Cleaned misclassified features saved to '{features_filename}'\")\n",
    "print(f\"File contains {len(misclassified_features_only)} rows and {len(misclassified_features_only.columns)} columns\")\n",
    "print(f\"Feature columns: {list(misclassified_features_only.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d071f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ Calculating average distance between correctly classified points...\n",
      "Number of correctly classified samples: 17933\n",
      "\n",
      "ðŸ§¹ Cleaning correctly classified features data...\n",
      "Before cleaning:\n",
      "  NaN values: 0\n",
      "  Infinite values: 0\n",
      "After cleaning:\n",
      "  NaN values: 0\n",
      "  Infinite values: 0\n",
      "Sampling 1000 correctly classified points for distance calculation...\n",
      "Calculating distances for 1000 samples...\n",
      "\n",
      "âœ… BASELINE_CORRECT_DISTANCE = 9.341171\n",
      "ðŸ“Š Additional statistics:\n",
      "  Median distance: 8.887026\n",
      "  Standard deviation: 3.930843\n",
      "  Calculated from 499496 valid distance pairs\n",
      "  Distance range: [0.000110, 22.758952]\n",
      "\n",
      "ðŸ“ Average distance between correctly classified points: 9.341171\n",
      "\n",
      "ðŸ’¡ Use this value in your Streamlit app as the baseline distance.\n",
      "\n",
      "ðŸ“‹ Summary for Streamlit Integration:\n",
      "  - CSV file: 'misclassified_features.csv'\n",
      "  - Average baseline distance: 9.341171\n",
      "  - Number of misclassified samples: 591\n",
      "  - Features to use for distance calculation: ['Time_spent_Alone', 'Stage_fear', 'Social_event_attendance', 'Going_outside', 'Drained_after_socializing', 'Friends_circle_size', 'Post_frequency']\n"
     ]
    }
   ],
   "source": [
    "# Calculate average distance between correctly classified points\n",
    "print(f\"\\nðŸ“ Calculating average distance between correctly classified points...\")\n",
    "\n",
    "# Get correctly classified samples\n",
    "correctly_classified_mask = y_pred == y_train_encoded\n",
    "correctly_classified_samples = X_train_encoded[correctly_classified_mask]\n",
    "\n",
    "print(f\"Number of correctly classified samples: {len(correctly_classified_samples)}\")\n",
    "\n",
    "# Clean the correctly classified features data\n",
    "print(f\"\\nðŸ§¹ Cleaning correctly classified features data...\")\n",
    "correct_features_df = pd.DataFrame(correctly_classified_samples, columns=feature_columns)\n",
    "\n",
    "print(f\"Before cleaning:\")\n",
    "print(f\"  NaN values: {correct_features_df.isnull().sum().sum()}\")\n",
    "print(f\"  Infinite values: {np.isinf(correct_features_df.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "\n",
    "# Handle NaN values\n",
    "for col in feature_columns:\n",
    "    if correct_features_df[col].isnull().any():\n",
    "        if col in numerical_columns:\n",
    "            median_val = correct_features_df[col].median()\n",
    "            if pd.isna(median_val):\n",
    "                median_val = 0\n",
    "            correct_features_df[col].fillna(median_val, inplace=True)\n",
    "        else:\n",
    "            mode_val = correct_features_df[col].mode()\n",
    "            if len(mode_val) > 0:\n",
    "                correct_features_df[col].fillna(mode_val[0], inplace=True)\n",
    "            else:\n",
    "                correct_features_df[col].fillna(0, inplace=True)\n",
    "\n",
    "# Handle infinite values\n",
    "numerical_cols = correct_features_df.select_dtypes(include=[np.number]).columns\n",
    "for col in numerical_cols:\n",
    "    inf_mask = np.isinf(correct_features_df[col])\n",
    "    if inf_mask.any():\n",
    "        correct_features_df.loc[inf_mask, col] = 0\n",
    "\n",
    "print(f\"After cleaning:\")\n",
    "print(f\"  NaN values: {correct_features_df.isnull().sum().sum()}\")\n",
    "print(f\"  Infinite values: {np.isinf(correct_features_df.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "\n",
    "# Convert back to numpy array for distance calculations\n",
    "correctly_classified_samples = correct_features_df.values\n",
    "\n",
    "# Calculate pairwise distances (sample a subset if too large)\n",
    "max_samples = 1000  # Limit for computational efficiency\n",
    "if len(correctly_classified_samples) > max_samples:\n",
    "    print(f\"Sampling {max_samples} correctly classified points for distance calculation...\")\n",
    "    sample_indices = np.random.choice(len(correctly_classified_samples), max_samples, replace=False)\n",
    "    sample_points = correctly_classified_samples[sample_indices]\n",
    "else:\n",
    "    sample_points = correctly_classified_samples\n",
    "\n",
    "print(f\"Calculating distances for {len(sample_points)} samples...\")\n",
    "\n",
    "# Calculate distances safely using only valid pairs\n",
    "def safe_euclidean_distance_matrix(X):\n",
    "    \"\"\"Calculate pairwise euclidean distances with NaN/inf protection\"\"\"\n",
    "    n = len(X)\n",
    "    distances = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            try:\n",
    "                # Check for any remaining NaN or inf values\n",
    "                point1, point2 = X[i], X[j]\n",
    "                \n",
    "                if np.any(np.isnan(point1)) or np.any(np.isnan(point2)) or \\\n",
    "                   np.any(np.isinf(point1)) or np.any(np.isinf(point2)):\n",
    "                    continue  # Skip this pair\n",
    "                \n",
    "                dist = np.sqrt(np.sum((point1 - point2) ** 2))\n",
    "                \n",
    "                if np.isfinite(dist) and dist > 0:  # Only add valid, positive distances\n",
    "                    distances.append(dist)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                continue  # Skip problematic pairs\n",
    "    \n",
    "    return np.array(distances)\n",
    "\n",
    "distances = safe_euclidean_distance_matrix(sample_points)\n",
    "\n",
    "if len(distances) > 0:\n",
    "    avg_correct_distance = np.mean(distances)\n",
    "    median_correct_distance = np.median(distances)\n",
    "    std_correct_distance = np.std(distances)\n",
    "    \n",
    "    print(f\"\\nâœ… BASELINE_CORRECT_DISTANCE = {avg_correct_distance:.6f}\")\n",
    "    print(f\"ðŸ“Š Additional statistics:\")\n",
    "    print(f\"  Median distance: {median_correct_distance:.6f}\")\n",
    "    print(f\"  Standard deviation: {std_correct_distance:.6f}\")\n",
    "    print(f\"  Calculated from {len(distances)} valid distance pairs\")\n",
    "    print(f\"  Distance range: [{np.min(distances):.6f}, {np.max(distances):.6f}]\")\n",
    "else:\n",
    "    print(\"âŒ Could not calculate baseline distance - no valid distance pairs found\")\n",
    "    avg_correct_distance = 1.0  # Fallback value\n",
    "\n",
    "# Save the average distance as a constant for the Streamlit app\n",
    "print(f\"\\nðŸ“ Average distance between correctly classified points: {avg_correct_distance:.6f}\")\n",
    "print(f\"\\nðŸ’¡ Use this value in your Streamlit app as the baseline distance.\")\n",
    "print(f\"\\nðŸ“‹ Summary for Streamlit Integration:\")\n",
    "print(f\"  - CSV file: '{features_filename}'\")\n",
    "print(f\"  - Average baseline distance: {avg_correct_distance:.6f}\")\n",
    "print(f\"  - Number of misclassified samples: {len(misclassified_df)}\")\n",
    "print(f\"  - Features to use for distance calculation: {feature_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65dbbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class-specific statistics for Streamlit\n",
    "print(\"\\nðŸ“Š Calculating class-specific statistics...\")\n",
    "\n",
    "# Overall statistics\n",
    "overall_stats = {\n",
    "    'total_samples': total_samples,\n",
    "    'correctly_classified': num_correctly_classified,\n",
    "    'misclassified': num_misclassified,\n",
    "    'overall_accuracy': num_correctly_classified / total_samples\n",
    "}\n",
    "\n",
    "print(f\"\\nðŸ“ˆ Overall Statistics:\")\n",
    "print(f\"  Total samples: {overall_stats['total_samples']}\")\n",
    "print(f\"  Correctly classified: {overall_stats['correctly_classified']} ({overall_stats['correctly_classified']/overall_stats['total_samples']*100:.1f}%)\")\n",
    "print(f\"  Misclassified: {overall_stats['misclassified']} ({overall_stats['misclassified']/overall_stats['total_samples']*100:.1f}%)\")\n",
    "print(f\"  Overall accuracy: {overall_stats['overall_accuracy']:.4f}\")\n",
    "\n",
    "# Class-specific statistics\n",
    "class_stats = {}\n",
    "for class_name in target_encoder.classes_:\n",
    "    class_encoded = target_encoder.transform([class_name])[0]\n",
    "    \n",
    "    # Samples where true label is this class\n",
    "    true_class_mask = y_train_encoded == class_encoded\n",
    "    true_class_total = np.sum(true_class_mask)\n",
    "    \n",
    "    # Correctly classified samples of this class\n",
    "    true_class_correct = np.sum(true_class_mask & correctly_classified_mask)\n",
    "    \n",
    "    # Misclassified samples of this class\n",
    "    true_class_misclassified = np.sum(true_class_mask & misclassified_mask)\n",
    "    \n",
    "    # Class-specific accuracy\n",
    "    class_accuracy = true_class_correct / true_class_total if true_class_total > 0 else 0\n",
    "    \n",
    "    class_stats[class_name] = {\n",
    "        'total_samples': int(true_class_total),\n",
    "        'correctly_classified': int(true_class_correct),\n",
    "        'misclassified': int(true_class_misclassified),\n",
    "        'class_accuracy': float(class_accuracy)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nðŸ“Š {class_name} Statistics:\")\n",
    "    print(f\"  Total {class_name} samples: {true_class_total}\")\n",
    "    print(f\"  Correctly classified: {true_class_correct} ({true_class_correct/true_class_total*100:.1f}%)\")\n",
    "    print(f\"  Misclassified: {true_class_misclassified} ({true_class_misclassified/true_class_total*100:.1f}%)\")\n",
    "    print(f\"  Class accuracy: {class_accuracy:.4f}\")\n",
    "\n",
    "# Save statistics for Streamlit\n",
    "import json\n",
    "\n",
    "statistics = {\n",
    "    'overall': overall_stats,\n",
    "    'by_class': class_stats,\n",
    "    'baseline_distance': float(avg_correct_distance)\n",
    "}\n",
    "\n",
    "stats_filename = 'model_statistics.json'\n",
    "with open(stats_filename, 'w') as f:\n",
    "    json.dump(statistics, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ… Statistics saved to '{stats_filename}'\")\n",
    "print(f\"\\nðŸ“‹ Statistics for Streamlit Integration:\")\n",
    "print(f\"  - Overall accuracy: {overall_stats['overall_accuracy']:.4f}\")\n",
    "print(f\"  - Baseline distance: {avg_correct_distance:.6f}\")\n",
    "print(f\"  - Class-specific stats available for: {list(class_stats.keys())}\")\n",
    "print(f\"  - Use model_statistics.json to load all stats in Streamlit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a233d84",
   "metadata": {},
   "source": [
    "## 7. Test Distance Calculation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "96fde54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§ª Testing distance calculation function...\n",
      "Test sample shape: (7,)\n",
      "Misclassified features shape: (591, 7)\n",
      "Test sample has NaN: False\n",
      "Test sample has inf: False\n",
      "Misclassified features has NaN: False\n",
      "Misclassified features has inf: False\n",
      "Test sample distance to nearest misclassified: 1.4142\n",
      "Nearest misclassified sample index: 532\n",
      "Baseline average distance: 9.3412\n",
      "Ratio (test_distance / baseline): 0.15\n",
      "âš ï¸ This test sample is closer to misclassified samples than the average distance between correct samples!\n",
      "\n",
      "ðŸ§ª Testing CSV file loading...\n",
      "Loaded CSV shape: (591, 7)\n",
      "Loaded CSV has NaN: False\n",
      "Loaded CSV has inf: False\n",
      "âœ… CSV file is clean and ready for Streamlit integration\n"
     ]
    }
   ],
   "source": [
    "# Test the distance calculation that will be used in Streamlit\n",
    "def calculate_distance_to_misclassified(user_input_array, misclassified_features):\n",
    "    \"\"\"Calculate the distance from user input to nearest misclassified sample\"\"\"\n",
    "    if len(misclassified_features) == 0:\n",
    "        return None, None\n",
    "    \n",
    "    # Convert user input to array if it's not already\n",
    "    user_array = np.array(user_input_array).reshape(1, -1)\n",
    "    \n",
    "    # Check for NaN or infinite values\n",
    "    if np.isnan(user_array).any() or np.isinf(user_array).any():\n",
    "        print(\"âš ï¸ User input contains NaN or infinite values\")\n",
    "        return None, None\n",
    "        \n",
    "    if np.isnan(misclassified_features).any() or np.isinf(misclassified_features).any():\n",
    "        print(\"âš ï¸ Misclassified features contain NaN or infinite values\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        # Calculate distances to all misclassified samples\n",
    "        distances = euclidean_distances(user_array, misclassified_features)\n",
    "        \n",
    "        # Find minimum distance and its index\n",
    "        min_distance = np.min(distances)\n",
    "        min_index = np.argmin(distances)\n",
    "        \n",
    "        return min_distance, min_index\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error calculating distances: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Test with a sample from the training data\n",
    "print(\"\\nðŸ§ª Testing distance calculation function...\")\n",
    "\n",
    "if len(misclassified_df) > 0 and 'misclassified_features_only' in locals():\n",
    "    # Use a clean sample for testing\n",
    "    test_sample = X_train_encoded.iloc[0][feature_columns].values\n",
    "    \n",
    "    # Clean the test sample\n",
    "    if np.isnan(test_sample).any() or np.isinf(test_sample).any():\n",
    "        print(\"âš ï¸ Test sample contains NaN/infinite values, cleaning...\")\n",
    "        # Replace NaN and inf with 0 for testing\n",
    "        test_sample = np.nan_to_num(test_sample, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    # Use the cleaned misclassified features\n",
    "    misclassified_features_clean = misclassified_features_only.values\n",
    "    \n",
    "    print(f\"Test sample shape: {test_sample.shape}\")\n",
    "    print(f\"Misclassified features shape: {misclassified_features_clean.shape}\")\n",
    "    print(f\"Test sample has NaN: {np.isnan(test_sample).any()}\")\n",
    "    print(f\"Test sample has inf: {np.isinf(test_sample).any()}\")\n",
    "    print(f\"Misclassified features has NaN: {np.isnan(misclassified_features_clean).any()}\")\n",
    "    print(f\"Misclassified features has inf: {np.isinf(misclassified_features_clean).any()}\")\n",
    "    \n",
    "    min_dist, min_idx = calculate_distance_to_misclassified(test_sample, misclassified_features_clean)\n",
    "    \n",
    "    if min_dist is not None and avg_correct_distance > 0:\n",
    "        print(f\"Test sample distance to nearest misclassified: {min_dist:.4f}\")\n",
    "        print(f\"Nearest misclassified sample index: {min_idx}\")\n",
    "        print(f\"Baseline average distance: {avg_correct_distance:.4f}\")\n",
    "        print(f\"Ratio (test_distance / baseline): {min_dist / avg_correct_distance:.2f}\")\n",
    "        \n",
    "        if min_dist < avg_correct_distance:\n",
    "            print(\"âš ï¸ This test sample is closer to misclassified samples than the average distance between correct samples!\")\n",
    "        else:\n",
    "            print(\"âœ… This test sample is in a 'safe' region away from misclassified samples.\")\n",
    "    else:\n",
    "        print(\"âŒ Could not calculate test distances\")\n",
    "        if avg_correct_distance == 0:\n",
    "            print(\"   - Baseline distance is 0, check distance calculation\")\n",
    "        if min_dist is None:\n",
    "            print(\"   - Distance calculation returned None\")\n",
    "else:\n",
    "    print(\"âŒ No misclassified samples available for testing\")\n",
    "\n",
    "# Additional test: Try loading the saved CSV file\n",
    "try:\n",
    "    print(f\"\\nðŸ§ª Testing CSV file loading...\")\n",
    "    loaded_features = pd.read_csv(features_filename)\n",
    "    print(f\"Loaded CSV shape: {loaded_features.shape}\")\n",
    "    print(f\"Loaded CSV has NaN: {loaded_features.isnull().any().any()}\")\n",
    "    print(f\"Loaded CSV has inf: {np.isinf(loaded_features.select_dtypes(include=[np.number])).any().any()}\")\n",
    "    \n",
    "    if not loaded_features.isnull().any().any() and not np.isinf(loaded_features.select_dtypes(include=[np.number])).any().any():\n",
    "        print(\"âœ… CSV file is clean and ready for Streamlit integration\")\n",
    "    else:\n",
    "        print(\"âš ï¸ CSV file still has data quality issues\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading CSV file: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9f1992",
   "metadata": {},
   "source": [
    "## 8. Summary\n",
    "\n",
    "**Files Created:**\n",
    "1. `misclassified_samples.csv` - Full information about misclassified samples including metadata\n",
    "2. `misclassified_features.csv` - Only the feature values for distance calculations\n",
    "3. `enhanced_misclassified_samples.csv` - **NEW**: Features + metadata for showing users specific samples they're closest to\n",
    "4. `model_statistics.json` - Overall and class-specific model performance statistics\n",
    "\n",
    "**Key Values for Streamlit:**\n",
    "- Average distance between correctly classified points: Use this as baseline\n",
    "- Use the `misclassified_features.csv` file to calculate distances\n",
    "- Use `enhanced_misclassified_samples.csv` to show users which specific sample they're closest to\n",
    "- Use `model_statistics.json` for displaying overall and class-specific performance\n",
    "\n",
    "**Model Performance Summary:**\n",
    "- Overall accuracy: 96.8% (17,933 correct out of 18,524 total samples)\n",
    "- Extrovert accuracy: 98.2% (13,449 correct out of 13,699 samples)\n",
    "- Introvert accuracy: 92.9% (4,484 correct out of 4,825 samples)\n",
    "- Total misclassified: 591 samples\n",
    "\n",
    "**New Enhanced Features:**\n",
    "- **Closest Misclassified Sample Display**: Users can see exactly which misclassified sample they are most similar to\n",
    "- **Feature-by-Feature Comparison**: Shows how the user's input compares to the closest misclassified sample\n",
    "- **Similarity Analysis**: Displays which features are most/least similar to problematic samples\n",
    "- **Risk Assessment**: Provides detailed insights into why a prediction might be reliable or unreliable\n",
    "\n",
    "**Integration Notes:**\n",
    "- Load `misclassified_features.csv` in Streamlit for distance calculations\n",
    "- Load `enhanced_misclassified_samples.csv` for detailed sample comparison\n",
    "- Load `model_statistics.json` for displaying statistics\n",
    "- Use the average distance value as a hardcoded constant\n",
    "- When user makes a prediction:\n",
    "  1. Calculate their distance to nearest misclassified sample\n",
    "  2. Show which specific sample they're closest to\n",
    "  3. Display feature-by-feature comparison\n",
    "  4. Compare distance to baseline to determine risk level\n",
    "- Display overall and class-specific statistics to give users context about model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0fd7bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ª Testing data integrity and distance calculations...\n",
      "\n",
      "1. Testing CSV loading...\n",
      "   âœ… Successfully loaded CSV with 591 rows and 7 columns\n",
      "\n",
      "2. Checking data quality...\n",
      "   NaN values in CSV: 0\n",
      "   Infinite values in CSV: 0\n",
      "   âœ… CSV data is clean\n",
      "\n",
      "3. Testing distance calculation...\n",
      "   Test distance calculation: 0.000000\n",
      "   âœ… Distance calculation working correctly\n",
      "\n",
      "4. Summary:\n",
      "   Total misclassified samples: 591\n",
      "   Feature columns: 7\n",
      "   Baseline correct distance: 9.341171\n",
      "   Data ready for Streamlit app: âœ…\n"
     ]
    }
   ],
   "source": [
    "# Test function to verify data integrity and CSV loading\n",
    "def test_distance_calculation():\n",
    "    \"\"\"Test the distance calculation and CSV data integrity\"\"\"\n",
    "    print(\"ðŸ§ª Testing data integrity and distance calculations...\")\n",
    "    \n",
    "    try:\n",
    "        # Test CSV loading\n",
    "        print(\"\\n1. Testing CSV loading...\")\n",
    "        test_df = pd.read_csv('misclassified_features.csv')\n",
    "        print(f\"   âœ… Successfully loaded CSV with {len(test_df)} rows and {len(test_df.columns)} columns\")\n",
    "        \n",
    "        # Check for data quality issues\n",
    "        print(\"\\n2. Checking data quality...\")\n",
    "        nan_count = test_df.isnull().sum().sum()\n",
    "        inf_count = np.isinf(test_df.select_dtypes(include=[np.number])).sum().sum()\n",
    "        \n",
    "        print(f\"   NaN values in CSV: {nan_count}\")\n",
    "        print(f\"   Infinite values in CSV: {inf_count}\")\n",
    "        \n",
    "        if nan_count == 0 and inf_count == 0:\n",
    "            print(\"   âœ… CSV data is clean\")\n",
    "        else:\n",
    "            print(\"   âš ï¸ CSV data has quality issues\")\n",
    "        \n",
    "        # Test distance calculation with a sample\n",
    "        print(\"\\n3. Testing distance calculation...\")\n",
    "        if len(test_df) >= 2:\n",
    "            test_sample = test_df.iloc[0].values\n",
    "            misclassified_features_clean = test_df.values\n",
    "            \n",
    "            # Test distance calculation\n",
    "            def calculate_distance_to_misclassified(user_features, misclassified_features):\n",
    "                distances = []\n",
    "                for i, mis_features in enumerate(misclassified_features):\n",
    "                    try:\n",
    "                        if np.any(np.isnan(user_features)) or np.any(np.isnan(mis_features)) or \\\n",
    "                           np.any(np.isinf(user_features)) or np.any(np.isinf(mis_features)):\n",
    "                            continue\n",
    "                        \n",
    "                        dist = np.sqrt(np.sum((user_features - mis_features) ** 2))\n",
    "                        if np.isfinite(dist):\n",
    "                            distances.append(dist)\n",
    "                    except:\n",
    "                        continue\n",
    "                \n",
    "                return min(distances) if distances else float('inf')\n",
    "            \n",
    "            min_distance = calculate_distance_to_misclassified(test_sample, misclassified_features_clean)\n",
    "            print(f\"   Test distance calculation: {min_distance:.6f}\")\n",
    "            \n",
    "            if np.isfinite(min_distance):\n",
    "                print(\"   âœ… Distance calculation working correctly\")\n",
    "            else:\n",
    "                print(\"   âŒ Distance calculation failed\")\n",
    "        \n",
    "        print(f\"\\n4. Summary:\")\n",
    "        print(f\"   Total misclassified samples: {len(test_df)}\")\n",
    "        print(f\"   Feature columns: {len(test_df.columns)}\")\n",
    "        print(f\"   Baseline correct distance: {avg_correct_distance:.6f}\")\n",
    "        print(f\"   Data ready for Streamlit app: {'âœ…' if nan_count == 0 and inf_count == 0 else 'âŒ'}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Test failed with error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# Run the test\n",
    "test_distance_calculation()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51a9093",
   "metadata": {},
   "source": [
    "## ðŸ“Š Model Statistics and Class-Specific Analysis\n",
    "\n",
    "In this section, we'll compute and save overall and class-specific statistics about model performance, including misclassification rates by personality type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "843e6fad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Computing overall model statistics...\n",
      "Total training samples: 18524\n",
      "Correctly classified: 17933\n",
      "Misclassified: 591\n",
      "Overall accuracy: 0.9681\n",
      "Overall misclassification rate: 0.0319\n",
      "âœ… Sample counts verified\n"
     ]
    }
   ],
   "source": [
    "# Calculate overall model statistics\n",
    "import json\n",
    "\n",
    "print(\"ðŸ“Š Computing overall model statistics...\")\n",
    "\n",
    "# Total samples\n",
    "total_samples = len(y_train)\n",
    "correctly_classified_count = np.sum(correctly_classified_mask)\n",
    "misclassified_count = np.sum(misclassified_mask)\n",
    "\n",
    "# Overall accuracy (should match what we computed before)\n",
    "overall_accuracy = correctly_classified_count / total_samples\n",
    "\n",
    "print(f\"Total training samples: {total_samples}\")\n",
    "print(f\"Correctly classified: {correctly_classified_count}\")\n",
    "print(f\"Misclassified: {misclassified_count}\")\n",
    "print(f\"Overall accuracy: {overall_accuracy:.4f}\")\n",
    "print(f\"Overall misclassification rate: {1 - overall_accuracy:.4f}\")\n",
    "\n",
    "# Verify our counts are correct\n",
    "assert correctly_classified_count + misclassified_count == total_samples, \"Sample counts don't add up!\"\n",
    "print(\"âœ… Sample counts verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b687b558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“ˆ Computing class-specific statistics...\n",
      "Number of personality types: 2\n",
      "Personality types: ['Extrovert', 'Introvert']\n",
      "\n",
      "Extrovert:\n",
      "  Total samples: 13699\n",
      "  Correctly classified: 13449\n",
      "  Misclassified: 250\n",
      "  Accuracy: 0.9818\n",
      "  Misclassification rate: 0.0182\n",
      "\n",
      "Introvert:\n",
      "  Total samples: 4825\n",
      "  Correctly classified: 4484\n",
      "  Misclassified: 341\n",
      "  Accuracy: 0.9293\n",
      "  Misclassification rate: 0.0707\n",
      "\n",
      "âœ… All class-specific statistics computed and verified\n"
     ]
    }
   ],
   "source": [
    "# Calculate class-specific statistics\n",
    "print(\"\\nðŸ“ˆ Computing class-specific statistics...\")\n",
    "\n",
    "# Get unique classes and their labels\n",
    "unique_classes = np.unique(y_train_encoded)\n",
    "class_labels = target_encoder.inverse_transform(unique_classes)\n",
    "\n",
    "print(f\"Number of personality types: {len(unique_classes)}\")\n",
    "print(f\"Personality types: {list(class_labels)}\")\n",
    "\n",
    "# Initialize dictionaries to store class-specific stats\n",
    "class_stats = {}\n",
    "\n",
    "for class_idx, class_label in zip(unique_classes, class_labels):\n",
    "    # Get mask for samples of this class\n",
    "    class_mask = (y_train_encoded == class_idx)\n",
    "    \n",
    "    # Count total samples for this class\n",
    "    total_class_samples = np.sum(class_mask)\n",
    "    \n",
    "    # Count correctly classified samples for this class\n",
    "    correct_class_samples = np.sum(class_mask & correctly_classified_mask)\n",
    "    \n",
    "    # Count misclassified samples for this class\n",
    "    misclassified_class_samples = np.sum(class_mask & misclassified_mask)\n",
    "    \n",
    "    # Calculate class-specific accuracy\n",
    "    class_accuracy = correct_class_samples / total_class_samples if total_class_samples > 0 else 0\n",
    "    class_misclassification_rate = misclassified_class_samples / total_class_samples if total_class_samples > 0 else 0\n",
    "    \n",
    "    # Store in dictionary\n",
    "    class_stats[class_label] = {\n",
    "        'total_samples': int(total_class_samples),\n",
    "        'correctly_classified': int(correct_class_samples),\n",
    "        'misclassified': int(misclassified_class_samples),\n",
    "        'accuracy': float(class_accuracy),\n",
    "        'misclassification_rate': float(class_misclassification_rate)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{class_label}:\")\n",
    "    print(f\"  Total samples: {total_class_samples}\")\n",
    "    print(f\"  Correctly classified: {correct_class_samples}\")\n",
    "    print(f\"  Misclassified: {misclassified_class_samples}\")\n",
    "    print(f\"  Accuracy: {class_accuracy:.4f}\")\n",
    "    print(f\"  Misclassification rate: {class_misclassification_rate:.4f}\")\n",
    "    \n",
    "    # Verify counts add up\n",
    "    assert correct_class_samples + misclassified_class_samples == total_class_samples, f\"Counts don't add up for {class_label}!\"\n",
    "\n",
    "print(\"\\nâœ… All class-specific statistics computed and verified\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77d0b183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ’¾ Preparing statistics for export...\n",
      "âœ… Model statistics saved to model_statistics.json\n",
      "   - Overall accuracy: 0.9681\n",
      "   - Total classes: 2\n",
      "   - Baseline distance: 9.341171\n",
      "   - File size: 969 characters\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive statistics dictionary\n",
    "print(\"\\nðŸ’¾ Preparing statistics for export...\")\n",
    "\n",
    "model_statistics = {\n",
    "    'overall': {\n",
    "        'total_samples': int(total_samples),\n",
    "        'correctly_classified': int(correctly_classified_count),\n",
    "        'misclassified': int(misclassified_count),\n",
    "        'accuracy': float(overall_accuracy),\n",
    "        'misclassification_rate': float(1 - overall_accuracy)\n",
    "    },\n",
    "    'by_class': class_stats,\n",
    "    'distance_metrics': {\n",
    "        'average_correct_distance': float(avg_correct_distance),\n",
    "        'median_correct_distance': float(median_correct_distance),\n",
    "        'std_correct_distance': float(std_correct_distance)\n",
    "    },\n",
    "    'metadata': {\n",
    "        'num_classes': len(unique_classes),\n",
    "        'class_labels': list(class_labels),\n",
    "        'feature_columns': feature_columns,\n",
    "        'total_misclassified_samples_saved': len(misclassified_df)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save to JSON file\n",
    "stats_filename = 'model_statistics.json'\n",
    "with open(stats_filename, 'w') as f:\n",
    "    json.dump(model_statistics, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Model statistics saved to {stats_filename}\")\n",
    "print(f\"   - Overall accuracy: {overall_accuracy:.4f}\")\n",
    "print(f\"   - Total classes: {len(unique_classes)}\")\n",
    "print(f\"   - Baseline distance: {avg_correct_distance:.6f}\")\n",
    "print(f\"   - File size: {len(json.dumps(model_statistics))} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0401f9d",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Enhanced Misclassified Samples for User Comparison\n",
    "\n",
    "Create an enhanced version of misclassified samples that includes both features and metadata for showing users which specific sample they are closest to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "956821f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‹ Creating enhanced misclassified samples file...\n",
      "âœ… Enhanced misclassified samples saved to 'enhanced_misclassified_samples.csv'\n",
      "   - Contains 591 samples\n",
      "   - Includes features + metadata for user comparison\n",
      "   - Columns: ['sample_id', 'original_index', 'true_label', 'predicted_label', 'confidence', 'Time_spent_Alone', 'Stage_fear', 'Social_event_attendance', 'Going_outside', 'Drained_after_socializing', 'Friends_circle_size', 'Post_frequency', 'description']\n",
      "\n",
      "ðŸ” Data quality check:\n",
      "   - NaN values: 0\n",
      "   - Infinite values: 0\n",
      "\n",
      "ðŸ“Š Sample of enhanced data:\n",
      "   sample_id true_label predicted_label  confidence  \\\n",
      "0          0  Extrovert       Introvert    0.821004   \n",
      "1          1  Extrovert       Introvert    0.901223   \n",
      "2          2  Introvert       Extrovert    0.890060   \n",
      "3          3  Extrovert       Introvert    0.818464   \n",
      "4          4  Introvert       Extrovert    0.892519   \n",
      "\n",
      "                                         description  \n",
      "0  Sample #0: True=Extrovert, Predicted=Introvert...  \n",
      "1  Sample #1: True=Extrovert, Predicted=Introvert...  \n",
      "2  Sample #2: True=Introvert, Predicted=Extrovert...  \n",
      "3  Sample #3: True=Extrovert, Predicted=Introvert...  \n",
      "4  Sample #4: True=Introvert, Predicted=Extrovert...  \n"
     ]
    }
   ],
   "source": [
    "# Create enhanced misclassified samples file for user comparison\n",
    "print(\"\\nðŸ“‹ Creating enhanced misclassified samples file...\")\n",
    "\n",
    "# Create a comprehensive misclassified samples file that includes both features and metadata\n",
    "enhanced_misclassified = []\n",
    "\n",
    "for idx, row in misclassified_df.iterrows():\n",
    "    # Get the original sample info\n",
    "    sample_data = {\n",
    "        'sample_id': idx,\n",
    "        'original_index': row['original_index'],\n",
    "        'true_label': row['true_label'],\n",
    "        'predicted_label': row['predicted_label'],\n",
    "        'confidence': row['confidence']\n",
    "    }\n",
    "    \n",
    "    # Add all feature values\n",
    "    for feature in feature_columns:\n",
    "        sample_data[feature] = row[feature]\n",
    "    \n",
    "    # Add some human-readable descriptions\n",
    "    sample_data['description'] = f\"Sample #{idx}: True={row['true_label']}, Predicted={row['predicted_label']}, Confidence={row['confidence']:.2f}\"\n",
    "    \n",
    "    enhanced_misclassified.append(sample_data)\n",
    "\n",
    "enhanced_misclassified_df = pd.DataFrame(enhanced_misclassified)\n",
    "\n",
    "# Save the enhanced version\n",
    "enhanced_filename = 'enhanced_misclassified_samples.csv'\n",
    "enhanced_misclassified_df.to_csv(enhanced_filename, index=False)\n",
    "\n",
    "print(f\"âœ… Enhanced misclassified samples saved to '{enhanced_filename}'\")\n",
    "print(f\"   - Contains {len(enhanced_misclassified_df)} samples\")\n",
    "print(f\"   - Includes features + metadata for user comparison\")\n",
    "print(f\"   - Columns: {list(enhanced_misclassified_df.columns)}\")\n",
    "\n",
    "# Verify the enhanced file has clean data\n",
    "print(f\"\\nðŸ” Data quality check:\")\n",
    "print(f\"   - NaN values: {enhanced_misclassified_df.isnull().sum().sum()}\")\n",
    "print(f\"   - Infinite values: {np.isinf(enhanced_misclassified_df.select_dtypes(include=[np.number])).sum().sum()}\")\n",
    "\n",
    "# Show a sample\n",
    "print(f\"\\nðŸ“Š Sample of enhanced data:\")\n",
    "print(enhanced_misclassified_df[['sample_id', 'true_label', 'predicted_label', 'confidence', 'description']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0d4d2cd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ðŸŽ‰ COMPLETION SUMMARY\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ FILES CREATED:\n",
      "   âœ… misclassified_samples.csv - 591 misclassified samples with metadata\n",
      "   âœ… misclassified_features.csv - 591 samples for distance calculations\n",
      "   âœ… model_statistics.json - Model performance statistics\n",
      "\n",
      "ðŸ“Š OVERALL MODEL PERFORMANCE:\n",
      "   â€¢ Total training samples: 18,524\n",
      "   â€¢ Correctly classified: 17,933 (96.8%)\n",
      "   â€¢ Misclassified: 591 (3.2%)\n",
      "\n",
      "ðŸ“ˆ CLASS-SPECIFIC PERFORMANCE:\n",
      "   â€¢ Extrovert:\n",
      "     - Total: 13,699 samples\n",
      "     - Accuracy: 98.2%\n",
      "     - Misclassified: 250 samples\n",
      "   â€¢ Introvert:\n",
      "     - Total: 4,825 samples\n",
      "     - Accuracy: 92.9%\n",
      "     - Misclassified: 341 samples\n",
      "\n",
      "ðŸ“ DISTANCE METRICS:\n",
      "   â€¢ Average distance between correct samples: 9.341171\n",
      "   â€¢ Median distance: 8.887026\n",
      "   â€¢ Standard deviation: 3.930843\n",
      "\n",
      "ðŸš€ STREAMLIT INTEGRATION:\n",
      "   1. Load 'misclassified_features.csv' for distance calculations\n",
      "   2. Load 'model_statistics.json' for displaying performance statistics\n",
      "   3. Use baseline distance: 9.341171\n",
      "   4. Run: streamlit run streamlit_personality_app.py\n",
      "\n",
      "âœ… All data processing complete! Ready for Streamlit app integration.\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Final summary display\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸŽ‰ COMPLETION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nðŸ“ FILES CREATED:\")\n",
    "print(f\"   âœ… {misclassified_filename} - {len(misclassified_df)} misclassified samples with metadata\")\n",
    "print(f\"   âœ… {features_filename} - {len(misclassified_features_only)} samples for distance calculations\")\n",
    "print(f\"   âœ… {stats_filename} - Model performance statistics\")\n",
    "\n",
    "print(f\"\\nðŸ“Š OVERALL MODEL PERFORMANCE:\")\n",
    "print(f\"   â€¢ Total training samples: {total_samples:,}\")\n",
    "print(f\"   â€¢ Correctly classified: {correctly_classified_count:,} ({overall_accuracy:.1%})\")\n",
    "print(f\"   â€¢ Misclassified: {misclassified_count:,} ({1-overall_accuracy:.1%})\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ CLASS-SPECIFIC PERFORMANCE:\")\n",
    "for class_name, stats in class_stats.items():\n",
    "    print(f\"   â€¢ {class_name}:\")\n",
    "    print(f\"     - Total: {stats['total_samples']:,} samples\")\n",
    "    print(f\"     - Accuracy: {stats['accuracy']:.1%}\")\n",
    "    print(f\"     - Misclassified: {stats['misclassified']:,} samples\")\n",
    "\n",
    "print(f\"\\nðŸ“ DISTANCE METRICS:\")\n",
    "print(f\"   â€¢ Average distance between correct samples: {avg_correct_distance:.6f}\")\n",
    "print(f\"   â€¢ Median distance: {median_correct_distance:.6f}\")\n",
    "print(f\"   â€¢ Standard deviation: {std_correct_distance:.6f}\")\n",
    "\n",
    "print(f\"\\nðŸš€ STREAMLIT INTEGRATION:\")\n",
    "print(f\"   1. Load '{features_filename}' for distance calculations\")\n",
    "print(f\"   2. Load '{stats_filename}' for displaying performance statistics\")\n",
    "print(f\"   3. Use baseline distance: {avg_correct_distance:.6f}\")\n",
    "print(f\"   4. Run: streamlit run streamlit_personality_app.py\")\n",
    "\n",
    "print(f\"\\nâœ… All data processing complete! Ready for Streamlit app integration.\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
