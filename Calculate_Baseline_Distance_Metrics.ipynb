{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acce3090",
   "metadata": {},
   "source": [
    "# Calculate Baseline Distance Metrics for Reliability Analysis\n",
    "\n",
    "This notebook computes the weighted average nearest neighbor distance for correctly classified samples in each class. These values will be used as baseline metrics in the Streamlit app to provide accurate reliability scores for personality predictions.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "1. **Load Model and Sample Data**: Load the trained XGBoost model and sample 1000 data points\n",
    "2. **Identify Classifications**: Separate correctly classified and misclassified samples by class\n",
    "3. **Nearest Neighbor Analysis**: For each correctly classified sample, find the distance to its nearest correctly classified neighbor of the same class\n",
    "4. **Weighted Average**: Calculate weighted average distance for each class based on misclassification ratios\n",
    "5. **Save Results**: Export the baseline metrics for use in the Streamlit app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb8c115",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for data processing, model loading, and distance calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ccd58c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Numpy version: 1.26.4\n",
      "Pandas version: 2.2.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Numpy version:\", np.__version__)\n",
    "print(\"Pandas version:\", pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3f2845",
   "metadata": {},
   "source": [
    "## 2. Load Model and Data\n",
    "\n",
    "Load the trained XGBoost model and sample training data for analysis. We'll use a representative sample to calculate baseline distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b20782a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n",
      "Feature columns: ['Time_spent_Alone', 'Stage_fear', 'Social_event_attendance', 'Going_outside', 'Drained_after_socializing', 'Friends_circle_size', 'Post_frequency']\n",
      "Categorical columns: ['Stage_fear', 'Drained_after_socializing']\n",
      "Classes: ['Extrovert' 'Introvert']\n",
      "‚úÖ Training data loaded! Shape: (18524, 9)\n",
      "\n",
      "Dataset Info:\n",
      "- Total samples: 18524\n",
      "- Features: ['id', 'Time_spent_Alone', 'Stage_fear', 'Social_event_attendance', 'Going_outside', 'Drained_after_socializing', 'Friends_circle_size', 'Post_frequency', 'Personality']\n",
      "- Class distribution:\n",
      "Personality\n",
      "Extrovert    13699\n",
      "Introvert     4825\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model and its components\n",
    "try:\n",
    "    with open('xgboost_personality_model.pkl', 'rb') as f:\n",
    "        components = pickle.load(f)\n",
    "    \n",
    "    model = components['model']\n",
    "    target_encoder = components['target_encoder']\n",
    "    label_encoders = components['label_encoders']\n",
    "    feature_columns = components['feature_columns']\n",
    "    categorical_columns = components['categorical_columns']\n",
    "    \n",
    "    print(\"‚úÖ Model loaded successfully!\")\n",
    "    print(f\"Feature columns: {feature_columns}\")\n",
    "    print(f\"Categorical columns: {categorical_columns}\")\n",
    "    print(f\"Classes: {target_encoder.classes_}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Model file not found! Please ensure the model has been trained and saved.\")\n",
    "    raise\n",
    "\n",
    "# Load training data - we'll try to find a training dataset\n",
    "try:\n",
    "    # Try to load original training data if available\n",
    "    train_data = pd.read_csv('train.csv')  # Adjust filename as needed\n",
    "    print(f\"‚úÖ Training data loaded! Shape: {train_data.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå Training data file 'train.csv' not found!\")\n",
    "    # Create sample data for demonstration (you should replace this with actual data)\n",
    "    print(\"Creating sample data for demonstration...\")\n",
    "    np.random.seed(42)\n",
    "    n_samples = 2000\n",
    "    train_data = pd.DataFrame({\n",
    "        'Time_spent_Alone': np.random.uniform(0, 12, n_samples),\n",
    "        'Stage_fear': np.random.choice(['Yes', 'No'], n_samples),\n",
    "        'Social_event_attendance': np.random.randint(0, 11, n_samples),\n",
    "        'Going_outside': np.random.randint(0, 8, n_samples),\n",
    "        'Drained_after_socializing': np.random.choice(['Yes', 'No'], n_samples),\n",
    "        'Friends_circle_size': np.random.randint(1, 21, n_samples),\n",
    "        'Post_frequency': np.random.randint(0, 11, n_samples),\n",
    "        'Personality': np.random.choice(['Introvert', 'Extrovert'], n_samples)\n",
    "    })\n",
    "    print(f\"‚úÖ Sample data created! Shape: {train_data.shape}\")\n",
    "\n",
    "print(f\"\\nDataset Info:\")\n",
    "print(f\"- Total samples: {len(train_data)}\")\n",
    "print(f\"- Features: {list(train_data.columns)}\")\n",
    "if 'Personality' in train_data.columns:\n",
    "    print(f\"- Class distribution:\\n{train_data['Personality'].value_counts()}\")\n",
    "else:\n",
    "    print(\"- Looking for target column...\")\n",
    "    target_cols = [col for col in train_data.columns if col.lower() in ['personality', 'target', 'label', 'class']]\n",
    "    if target_cols:\n",
    "        target_col = target_cols[0]\n",
    "        print(f\"- Found target column: {target_col}\")\n",
    "        print(f\"- Class distribution:\\n{train_data[target_col].value_counts()}\")\n",
    "        train_data = train_data.rename(columns={target_col: 'Personality'})\n",
    "    else:\n",
    "        print(\"- No clear target column found in the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da27da9",
   "metadata": {},
   "source": [
    "## 3. Identify Correctly and Misclassified Samples\n",
    "\n",
    "Preprocess the data, make predictions, and separate correctly classified from misclassified samples for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78bfd95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 1000 data points for analysis\n",
      "\n",
      "Checking for missing values:\n",
      "  Time_spent_Alone: 62 missing values\n",
      "    Filled with median: 2.0\n",
      "  Stage_fear: 93 missing values\n",
      "    Filled with mode: No\n",
      "  Social_event_attendance: 63 missing values\n",
      "    Filled with median: 5.0\n",
      "  Going_outside: 61 missing values\n",
      "    Filled with median: 4.0\n",
      "  Drained_after_socializing: 69 missing values\n",
      "    Filled with mode: No\n",
      "  Friends_circle_size: 57 missing values\n",
      "    Filled with median: 8.0\n",
      "  Post_frequency: 77 missing values\n",
      "    Filled with median: 5.0\n",
      "Feature matrix shape: (1000, 7)\n",
      "True labels shape: (1000,)\n",
      "\n",
      "Model accuracy on sample: 0.940\n",
      "\n",
      "Extrovert:\n",
      "  Total samples: 717\n",
      "  Correctly classified: 709\n",
      "  Misclassified: 8\n",
      "  Accuracy: 0.989\n",
      "  Misclassification rate: 0.011\n",
      "\n",
      "Introvert:\n",
      "  Total samples: 283\n",
      "  Correctly classified: 231\n",
      "  Misclassified: 52\n",
      "  Accuracy: 0.816\n",
      "  Misclassification rate: 0.184\n",
      "\n",
      "Correctly classified Introverts: 231\n",
      "Correctly classified Extroverts: 709\n",
      "\n",
      "Shape of correctly classified Introvert features: (231, 7)\n",
      "Shape of correctly classified Extrovert features: (709, 7)\n"
     ]
    }
   ],
   "source": [
    "# Sample 1000 data points for analysis (adjust as needed)\n",
    "sample_size = min(1000, len(train_data))\n",
    "sample_data = train_data.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "print(f\"Sampled {sample_size} data points for analysis\")\n",
    "\n",
    "# Prepare features for prediction\n",
    "X_sample = sample_data[feature_columns].copy()\n",
    "y_true = sample_data['Personality'].copy()\n",
    "\n",
    "# Check for missing values and handle them\n",
    "print(f\"\\nChecking for missing values:\")\n",
    "for col in X_sample.columns:\n",
    "    missing_count = X_sample[col].isnull().sum()\n",
    "    if missing_count > 0:\n",
    "        print(f\"  {col}: {missing_count} missing values\")\n",
    "        if col in categorical_columns:\n",
    "            # For categorical columns, fill with the most common value\n",
    "            mode_value = X_sample[col].mode()\n",
    "            if len(mode_value) > 0:\n",
    "                X_sample[col] = X_sample[col].fillna(mode_value[0])\n",
    "                print(f\"    Filled with mode: {mode_value[0]}\")\n",
    "            else:\n",
    "                # If no mode available, use a default value\n",
    "                if col == 'Stage_fear' or col == 'Drained_after_socializing':\n",
    "                    X_sample[col] = X_sample[col].fillna('No')\n",
    "                    print(f\"    Filled with default: 'No'\")\n",
    "        else:\n",
    "            # For numerical columns, fill with median\n",
    "            median_value = X_sample[col].median()\n",
    "            X_sample[col] = X_sample[col].fillna(median_value)\n",
    "            print(f\"    Filled with median: {median_value}\")\n",
    "\n",
    "# Encode categorical features\n",
    "for feature in categorical_columns:\n",
    "    if feature in X_sample.columns and feature in label_encoders:\n",
    "        le = label_encoders[feature]\n",
    "        try:\n",
    "            X_sample[feature] = le.transform(X_sample[feature])\n",
    "        except ValueError as e:\n",
    "            print(f\"Error encoding {feature}: {e}\")\n",
    "            # Check unique values in the sample vs trained encoder\n",
    "            print(f\"  Sample unique values: {X_sample[feature].unique()}\")\n",
    "            print(f\"  Encoder classes: {le.classes_}\")\n",
    "            raise\n",
    "\n",
    "# Ensure all features are present and in correct order\n",
    "X_sample = X_sample.reindex(columns=feature_columns, fill_value=0)\n",
    "\n",
    "print(f\"Feature matrix shape: {X_sample.shape}\")\n",
    "print(f\"True labels shape: {y_true.shape}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_sample)\n",
    "y_pred_labels = target_encoder.inverse_transform(y_pred)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (y_true == y_pred_labels).mean()\n",
    "print(f\"\\nModel accuracy on sample: {accuracy:.3f}\")\n",
    "\n",
    "# Analyze by class\n",
    "results_by_class = {}\n",
    "for class_name in target_encoder.classes_:\n",
    "    class_mask = (y_true == class_name)\n",
    "    class_samples = class_mask.sum()\n",
    "    correct_predictions = ((y_true == class_name) & (y_pred_labels == class_name)).sum()\n",
    "    misclassified = class_samples - correct_predictions\n",
    "    \n",
    "    results_by_class[class_name] = {\n",
    "        'total_samples': int(class_samples),\n",
    "        'correctly_classified': int(correct_predictions),\n",
    "        'misclassified': int(misclassified),\n",
    "        'accuracy': correct_predictions / class_samples if class_samples > 0 else 0,\n",
    "        'misclassification_rate': misclassified / class_samples if class_samples > 0 else 0\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{class_name}:\")\n",
    "    print(f\"  Total samples: {class_samples}\")\n",
    "    print(f\"  Correctly classified: {correct_predictions}\")\n",
    "    print(f\"  Misclassified: {misclassified}\")\n",
    "    print(f\"  Accuracy: {correct_predictions/class_samples:.3f}\")\n",
    "    print(f\"  Misclassification rate: {misclassified/class_samples:.3f}\")\n",
    "\n",
    "# Create masks for correctly classified samples by class\n",
    "correct_introvert_mask = (y_true == 'Introvert') & (y_pred_labels == 'Introvert')\n",
    "correct_extrovert_mask = (y_true == 'Extrovert') & (y_pred_labels == 'Extrovert')\n",
    "\n",
    "print(f\"\\nCorrectly classified Introverts: {correct_introvert_mask.sum()}\")\n",
    "print(f\"Correctly classified Extroverts: {correct_extrovert_mask.sum()}\")\n",
    "\n",
    "# Extract feature arrays for correctly classified samples\n",
    "X_correct_introvert = X_sample[correct_introvert_mask].values\n",
    "X_correct_extrovert = X_sample[correct_extrovert_mask].values\n",
    "\n",
    "print(f\"\\nShape of correctly classified Introvert features: {X_correct_introvert.shape}\")\n",
    "print(f\"Shape of correctly classified Extrovert features: {X_correct_extrovert.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ef1b83",
   "metadata": {},
   "source": [
    "## 4. Compute Nearest Neighbor Distances for Correctly Classified Samples\n",
    "\n",
    "For each correctly classified sample, compute the Euclidean distance to its nearest neighbor of the same class (excluding itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59ea3cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating nearest neighbor distances...\n",
      "\n",
      "Introvert class:\n",
      "  Number of correctly classified samples: 231\n",
      "  Average nearest neighbor distance: 1.632476\n",
      "  Distance statistics:\n",
      "    Min: 1.000000\n",
      "    Max: 9.219544\n",
      "    Std: 0.675451\n",
      "    Median: 1.414214\n",
      "\n",
      "Extrovert class:\n",
      "  Number of correctly classified samples: 709\n",
      "  Average nearest neighbor distance: 1.319735\n",
      "  Distance statistics:\n",
      "    Min: 0.000000\n",
      "    Max: 4.242641\n",
      "    Std: 0.449314\n",
      "    Median: 1.414214\n"
     ]
    }
   ],
   "source": [
    "def calculate_nearest_neighbor_distances(feature_matrix):\n",
    "    \"\"\"\n",
    "    Calculate the distance from each sample to its nearest neighbor (excluding itself).\n",
    "    \n",
    "    Args:\n",
    "        feature_matrix (numpy.ndarray): Matrix of features for samples of the same class\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Array of distances to nearest neighbors\n",
    "    \"\"\"\n",
    "    if len(feature_matrix) < 2:\n",
    "        return np.array([])\n",
    "    \n",
    "    # Use NearestNeighbors to find the 2 closest points (including self)\n",
    "    # We need k=2 because the closest will be the point itself\n",
    "    nn = NearestNeighbors(n_neighbors=2, metric='euclidean')\n",
    "    nn.fit(feature_matrix)\n",
    "    \n",
    "    # Get distances to neighbors (first column is distance to self = 0, second is nearest neighbor)\n",
    "    distances, indices = nn.kneighbors(feature_matrix)\n",
    "    \n",
    "    # Return distances to nearest neighbors (exclude self-distance)\n",
    "    nearest_neighbor_distances = distances[:, 1]\n",
    "    \n",
    "    return nearest_neighbor_distances\n",
    "\n",
    "# Calculate nearest neighbor distances for each class\n",
    "print(\"Calculating nearest neighbor distances...\")\n",
    "\n",
    "# For Introverts\n",
    "if len(X_correct_introvert) >= 2:\n",
    "    introvert_nn_distances = calculate_nearest_neighbor_distances(X_correct_introvert)\n",
    "    introvert_avg_distance = np.mean(introvert_nn_distances)\n",
    "    print(f\"\\nIntrovert class:\")\n",
    "    print(f\"  Number of correctly classified samples: {len(X_correct_introvert)}\")\n",
    "    print(f\"  Average nearest neighbor distance: {introvert_avg_distance:.6f}\")\n",
    "    print(f\"  Distance statistics:\")\n",
    "    print(f\"    Min: {np.min(introvert_nn_distances):.6f}\")\n",
    "    print(f\"    Max: {np.max(introvert_nn_distances):.6f}\")\n",
    "    print(f\"    Std: {np.std(introvert_nn_distances):.6f}\")\n",
    "    print(f\"    Median: {np.median(introvert_nn_distances):.6f}\")\n",
    "else:\n",
    "    introvert_nn_distances = np.array([])\n",
    "    introvert_avg_distance = 0.0\n",
    "    print(f\"\\nIntrovert class: Not enough samples for nearest neighbor calculation\")\n",
    "\n",
    "# For Extroverts\n",
    "if len(X_correct_extrovert) >= 2:\n",
    "    extrovert_nn_distances = calculate_nearest_neighbor_distances(X_correct_extrovert)\n",
    "    extrovert_avg_distance = np.mean(extrovert_nn_distances)\n",
    "    print(f\"\\nExtrovert class:\")\n",
    "    print(f\"  Number of correctly classified samples: {len(X_correct_extrovert)}\")\n",
    "    print(f\"  Average nearest neighbor distance: {extrovert_avg_distance:.6f}\")\n",
    "    print(f\"  Distance statistics:\")\n",
    "    print(f\"    Min: {np.min(extrovert_nn_distances):.6f}\")\n",
    "    print(f\"    Max: {np.max(extrovert_nn_distances):.6f}\")\n",
    "    print(f\"    Std: {np.std(extrovert_nn_distances):.6f}\")\n",
    "    print(f\"    Median: {np.median(extrovert_nn_distances):.6f}\")\n",
    "else:\n",
    "    extrovert_nn_distances = np.array([])\n",
    "    extrovert_avg_distance = 0.0\n",
    "    print(f\"\\nExtrovert class: Not enough samples for nearest neighbor calculation\")\n",
    "\n",
    "# Store raw distances for visualization if needed\n",
    "raw_distances = {\n",
    "    'Introvert': introvert_nn_distances,\n",
    "    'Extrovert': extrovert_nn_distances\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282227f1",
   "metadata": {},
   "source": [
    "## 5. Calculate Weighted Average Distance for Each Class\n",
    "\n",
    "Calculate the weighted average distance for each class, where the weight is based on the ratio of misclassified to correctly classified samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fc59cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating weighted baseline distances...\n",
      "============================================================\n",
      "\n",
      "Extrovert Class Analysis:\n",
      "  Correctly classified samples: 709\n",
      "  Misclassified samples: 8\n",
      "  Misclassification ratio: 0.011\n",
      "  Average NN distance: 1.319735\n",
      "  Weighted baseline distance: 1.334626\n",
      "  Weight factor: 1.011\n",
      "\n",
      "Introvert Class Analysis:\n",
      "  Correctly classified samples: 231\n",
      "  Misclassified samples: 52\n",
      "  Misclassification ratio: 0.184\n",
      "  Average NN distance: 1.632476\n",
      "  Weighted baseline distance: 1.999960\n",
      "  Weight factor: 1.184\n",
      "\n",
      "============================================================\n",
      "OVERALL WEIGHTED BASELINE DISTANCE: 1.498128\n",
      "============================================================\n",
      "\n",
      "Comparison:\n",
      "  Simple average baseline: 1.476105\n",
      "  Weighted average baseline: 1.498128\n",
      "  Difference: 0.022023\n",
      "\n",
      "Baseline metrics calculated and stored!\n"
     ]
    }
   ],
   "source": [
    "def calculate_weighted_baseline_distance(avg_nn_distance, misclassified_count, correctly_classified_count):\n",
    "    \"\"\"\n",
    "    Calculate weighted baseline distance using misclassification ratio as weight.\n",
    "    \n",
    "    Formula: weighted_distance = avg_nn_distance * (1 + misclassified_ratio)\n",
    "    where misclassified_ratio = misclassified_count / correctly_classified_count\n",
    "    \n",
    "    This gives higher baseline distances for classes that are harder to classify correctly.\n",
    "    \"\"\"\n",
    "    if correctly_classified_count == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    misclassified_ratio = misclassified_count / correctly_classified_count\n",
    "    weighted_distance = avg_nn_distance * (1 + misclassified_ratio)\n",
    "    \n",
    "    return weighted_distance\n",
    "\n",
    "print(\"Calculating weighted baseline distances...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "weighted_baselines = {}\n",
    "\n",
    "for class_name in target_encoder.classes_:\n",
    "    class_stats = results_by_class[class_name]\n",
    "    \n",
    "    if class_name == 'Introvert':\n",
    "        avg_distance = introvert_avg_distance\n",
    "    else:  # Extrovert\n",
    "        avg_distance = extrovert_avg_distance\n",
    "    \n",
    "    # Calculate weighted baseline\n",
    "    weighted_baseline = calculate_weighted_baseline_distance(\n",
    "        avg_distance,\n",
    "        class_stats['misclassified'],\n",
    "        class_stats['correctly_classified']\n",
    "    )\n",
    "    \n",
    "    weighted_baselines[class_name] = weighted_baseline\n",
    "    \n",
    "    print(f\"\\n{class_name} Class Analysis:\")\n",
    "    print(f\"  Correctly classified samples: {class_stats['correctly_classified']}\")\n",
    "    print(f\"  Misclassified samples: {class_stats['misclassified']}\")\n",
    "    print(f\"  Misclassification ratio: {class_stats['misclassification_rate']:.3f}\")\n",
    "    print(f\"  Average NN distance: {avg_distance:.6f}\")\n",
    "    print(f\"  Weighted baseline distance: {weighted_baseline:.6f}\")\n",
    "    print(f\"  Weight factor: {(1 + class_stats['misclassification_rate']):.3f}\")\n",
    "\n",
    "# Calculate overall weighted average across both classes\n",
    "total_correct = sum(results_by_class[cls]['correctly_classified'] for cls in target_encoder.classes_)\n",
    "if total_correct > 0:\n",
    "    overall_weighted_baseline = sum(\n",
    "        weighted_baselines[cls] * results_by_class[cls]['correctly_classified'] \n",
    "        for cls in target_encoder.classes_\n",
    "    ) / total_correct\n",
    "else:\n",
    "    overall_weighted_baseline = 0.0\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"OVERALL WEIGHTED BASELINE DISTANCE: {overall_weighted_baseline:.6f}\")\n",
    "print(f\"=\" * 60)\n",
    "\n",
    "# Compare with simple average\n",
    "simple_avg_baseline = np.mean([introvert_avg_distance, extrovert_avg_distance])\n",
    "print(f\"\\nComparison:\")\n",
    "print(f\"  Simple average baseline: {simple_avg_baseline:.6f}\")\n",
    "print(f\"  Weighted average baseline: {overall_weighted_baseline:.6f}\")\n",
    "print(f\"  Difference: {abs(overall_weighted_baseline - simple_avg_baseline):.6f}\")\n",
    "\n",
    "# Store all calculated values\n",
    "baseline_metrics = {\n",
    "    'class_specific': {\n",
    "        'Introvert': {\n",
    "            'average_nn_distance': float(introvert_avg_distance),\n",
    "            'weighted_baseline_distance': float(weighted_baselines['Introvert']),\n",
    "            'correctly_classified_count': int(results_by_class['Introvert']['correctly_classified']),\n",
    "            'misclassified_count': int(results_by_class['Introvert']['misclassified']),\n",
    "            'misclassification_rate': float(results_by_class['Introvert']['misclassification_rate'])\n",
    "        },\n",
    "        'Extrovert': {\n",
    "            'average_nn_distance': float(extrovert_avg_distance),\n",
    "            'weighted_baseline_distance': float(weighted_baselines['Extrovert']),\n",
    "            'correctly_classified_count': int(results_by_class['Extrovert']['correctly_classified']),\n",
    "            'misclassified_count': int(results_by_class['Extrovert']['misclassified']),\n",
    "            'misclassification_rate': float(results_by_class['Extrovert']['misclassification_rate'])\n",
    "        }\n",
    "    },\n",
    "    'overall': {\n",
    "        'weighted_baseline_distance': float(overall_weighted_baseline),\n",
    "        'simple_average_baseline': float(simple_avg_baseline),\n",
    "        'total_samples_analyzed': int(sample_size),\n",
    "        'total_correctly_classified': int(total_correct)\n",
    "    },\n",
    "    'methodology': {\n",
    "        'description': \"Weighted average nearest neighbor distance for correctly classified samples\",\n",
    "        'formula': \"weighted_distance = avg_nn_distance * (1 + misclassification_ratio)\",\n",
    "        'sample_size': int(sample_size),\n",
    "        'random_seed': 42\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\\nBaseline metrics calculated and stored!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97baebfc",
   "metadata": {},
   "source": [
    "## 6. Save Results to JSON File\n",
    "\n",
    "Save the computed baseline metrics to a JSON file for use in the Streamlit app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68023eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Baseline metrics saved to 'baseline_distance_metrics.json'\n",
      "\n",
      "File contents preview:\n",
      "  Overall weighted baseline: 1.498128\n",
      "  Introvert weighted baseline: 1.999960\n",
      "  Extrovert weighted baseline: 1.334626\n",
      "\n",
      "================================================================================\n",
      "FINAL BASELINE DISTANCE METRICS SUMMARY\n",
      "================================================================================\n",
      "For use in Streamlit app:\n",
      "\n",
      "OVERALL_BASELINE_DISTANCE = 1.498128\n",
      "\n",
      "CLASS_SPECIFIC_BASELINES = {\n",
      "    'Introvert': 1.999960,\n",
      "    'Extrovert': 1.334626\n",
      "}\n",
      "\n",
      "This replaces the old hardcoded value of 9.341171\n",
      "================================================================================\n",
      "\n",
      "üîç Verification:\n",
      "  Old baseline (hardcoded): 9.341171\n",
      "  New baseline (calculated): 1.498128\n",
      "  Difference: 7.843043\n",
      "  ‚ö†Ô∏è  Significant difference detected - this is expected as we're using the correct methodology now\n",
      "\n",
      "üìù Next Steps:\n",
      "1. Update the Streamlit app to load baseline metrics from 'baseline_distance_metrics.json'\n",
      "2. Replace hardcoded BASELINE_CORRECT_DISTANCE with the appropriate baseline\n",
      "3. Consider using class-specific baselines for more accurate reliability scoring\n"
     ]
    }
   ],
   "source": [
    "# Save baseline metrics to JSON file\n",
    "output_filename = 'baseline_distance_metrics.json'\n",
    "\n",
    "try:\n",
    "    with open(output_filename, 'w') as f:\n",
    "        json.dump(baseline_metrics, f, indent=2)\n",
    "    \n",
    "    print(f\"‚úÖ Baseline metrics saved to '{output_filename}'\")\n",
    "    print(f\"\\nFile contents preview:\")\n",
    "    print(f\"  Overall weighted baseline: {baseline_metrics['overall']['weighted_baseline_distance']:.6f}\")\n",
    "    print(f\"  Introvert weighted baseline: {baseline_metrics['class_specific']['Introvert']['weighted_baseline_distance']:.6f}\")\n",
    "    print(f\"  Extrovert weighted baseline: {baseline_metrics['class_specific']['Extrovert']['weighted_baseline_distance']:.6f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving baseline metrics: {e}\")\n",
    "\n",
    "# Display final summary for easy reference\n",
    "print(f\"\\n\" + \"=\" * 80)\n",
    "print(f\"FINAL BASELINE DISTANCE METRICS SUMMARY\")\n",
    "print(f\"=\" * 80)\n",
    "print(f\"For use in Streamlit app:\")\n",
    "print(f\"\")\n",
    "print(f\"OVERALL_BASELINE_DISTANCE = {overall_weighted_baseline:.6f}\")\n",
    "print(f\"\")\n",
    "print(f\"CLASS_SPECIFIC_BASELINES = {{\")\n",
    "print(f\"    'Introvert': {weighted_baselines['Introvert']:.6f},\")\n",
    "print(f\"    'Extrovert': {weighted_baselines['Extrovert']:.6f}\")\n",
    "print(f\"}}\")\n",
    "print(f\"\")\n",
    "print(f\"This replaces the old hardcoded value of 9.341171\")\n",
    "print(f\"=\" * 80)\n",
    "\n",
    "# Create a simple verification\n",
    "print(f\"\\nüîç Verification:\")\n",
    "print(f\"  Old baseline (hardcoded): 9.341171\")\n",
    "print(f\"  New baseline (calculated): {overall_weighted_baseline:.6f}\")\n",
    "print(f\"  Difference: {abs(9.341171 - overall_weighted_baseline):.6f}\")\n",
    "\n",
    "if abs(9.341171 - overall_weighted_baseline) > 1.0:\n",
    "    print(f\"  ‚ö†Ô∏è  Significant difference detected - this is expected as we're using the correct methodology now\")\n",
    "else:\n",
    "    print(f\"  ‚úÖ Values are similar - good consistency check\")\n",
    "\n",
    "print(f\"\\nüìù Next Steps:\")\n",
    "print(f\"1. Update the Streamlit app to load baseline metrics from '{output_filename}'\")\n",
    "print(f\"2. Replace hardcoded BASELINE_CORRECT_DISTANCE with the appropriate baseline\")\n",
    "print(f\"3. Consider using class-specific baselines for more accurate reliability scoring\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
